{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "\n",
    "sys.path.append(\"../../../open_lm\")\n",
    "from open_lm.model import Transformer\n",
    "from open_lm.norms import RmsNorm\n",
    "\n",
    "device = \"cuda:0\"\n",
    "cfg = json.load(\n",
    "    open(\"../model_configs/l7b_llama.json\")\n",
    ")  # m1b_neox # l7b_neox # m1b_neox_384\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Params:\n",
    "    dim: int\n",
    "    n_layers: int\n",
    "    n_heads: int\n",
    "    vocab_size: int\n",
    "    norm_eps: float\n",
    "    seq_len: int\n",
    "    post_embed_norm: bool\n",
    "    weight_tying: bool\n",
    "    norm_type: nn.Module = RmsNorm\n",
    "    apply_qk_norm: bool = False\n",
    "    rotary_old: bool = False\n",
    "    ffn_type: str = \"swiglu\"\n",
    "\n",
    "\n",
    "args = Params(\n",
    "    dim=cfg[\"hidden_dim\"],\n",
    "    n_layers=cfg[\"n_layers\"],\n",
    "    n_heads=cfg[\"n_heads\"],\n",
    "    seq_len=cfg[\"seq_len\"],\n",
    "    vocab_size=cfg[\"vocab_size\"],\n",
    "    post_embed_norm=cfg[\"post_embed_norm\"],\n",
    "    weight_tying=cfg[\"weight_tying\"],\n",
    "    norm_eps=1e-5,\n",
    ")\n",
    "\n",
    "model = Transformer(args)\n",
    "state_dict = torch.load(\n",
    "    \"/pasteur/u/yuhuiz/data/LLAMA2/llama-2-7b/consolidated.00.converted.pth\"\n",
    ")\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "model = model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================\n",
      "I believe the meaning of life is\n",
      "I believe the meaning of life is to be happy, and I know that happiness is a choice. I choose happiness every day. I choose to be happy in spite of my circumstances. I choose to be happy in spite of the fact that I have a chronic illness. I choose to be happy in spite of the fact that I have a chronic illness that is incurable and untreatable. I choose to be happy in spite of the fact that I am not well.\n",
      "I choose to be happy in spite of the fact that I have to take 16 pills every day. I choose to be happy in spite of the fact that\n",
      "====================================\n",
      "====================================\n",
      "Simply put, the theory of relativity states that \n",
      "Simply put, the theory of relativity states that 1) the laws of physics are the same for all observers, and 2) the speed of light is the same for all observers, regardless of their state of motion.\n",
      "Both of these statements are contrary to the everyday experience of most people, who think that the laws of physics are different for different observers, and that the speed of light depends on the motion of the observer.\n",
      "The theory of relativity is based on two postulates:\n",
      "The laws of physics are the same for all observers.\n",
      "The speed of light is the same for all observers, regardless of their state of motion\n",
      "====================================\n",
      "====================================\n",
      "A brief message congratulating the team on the launch:\n",
      "\n",
      "    Hi everyone,\n",
      "    \n",
      "    I just \n",
      "A brief message congratulating the team on the launch:\n",
      "\n",
      "    Hi everyone,\n",
      "    \n",
      "    I just  wanted to congratulate the team on the launch of the site.\n",
      "    \n",
      "    I know it's been a long time coming, and you've all put in a lot of\n",
      "    hard work to get it here.\n",
      "    \n",
      "    I just wanted to say thanks for all your efforts, and I hope you all\n",
      "    enjoy the site.\n",
      "    \n",
      "    Best of luck,\n",
      "    \n",
      "    ---\n",
      "\n",
      "    Thanks,\n",
      "\n",
      "    ------\n",
      "\n",
      "    I just wanted to congratulate the team on the launch of the site.\n",
      "\n",
      "    I know it's been a\n",
      "====================================\n",
      "====================================\n",
      "Translate English to French:\n",
      "    \n",
      "    sea otter => loutre de mer\n",
      "    peppermint => menthe poivrée\n",
      "    plush girafe => girafe peluche\n",
      "    cheese =>\n",
      "Translate English to French:\n",
      "    \n",
      "    sea otter => loutre de mer\n",
      "    peppermint => menthe poivrée\n",
      "    plush girafe => girafe peluche\n",
      "    cheese => fromage\n",
      "    pizza => pizza\n",
      "    chicken => poulet\n",
      "    apple => pomme\n",
      "    hamburger => hamburger\n",
      "    mango => mangue\n",
      "    banana => banane\n",
      "    carrot => carotte\n",
      "    orange => orange\n",
      "    strawberry => fraise\n",
      "    beef => bœuf\n",
      "    pepper => poivre\n",
      "    broccoli => brocoli\n",
      "    potato => pomme de terre\n",
      "    onion => oignon\n",
      "    lettuce => laitue\n",
      "    tomato => tomate\n",
      "    garlic => ail\n",
      "\n",
      "====================================\n",
      "====================================\n",
      "He -> Him, She -> Her, They ->\n",
      "He -> Him, She -> Her, They -> Them\n",
      "The words he, she, they, him, her, and them are used to describe a person’s gender.\n",
      "In the past, these words were used to refer to people of all genders. But today, people often use them to refer to people of one gender only.\n",
      "For example, if you’re talking about a man, you might say, “He is a good person.”\n",
      "If you’re talking about a woman, you might say, “She is a good person.”\n",
      "And if you’re talking about a group of people, you might say, “They are good people\n",
      "====================================\n",
      "====================================\n",
      "Who is Donald Trump?\n",
      "Who is Donald Trump? A look at the businessman and presidential candidate\n",
      "Donald Trump is a businessman who is running for president of the United States.\n",
      "Donald Trump is a businessman who is running for president of the United States. (Carolyn Kaster / AP)\n",
      "Donald Trump is a businessman and reality TV star who is running for president of the United States.\n",
      "Trump is the Republican Party's presumptive nominee for president.\n",
      "He is the chairman and president of The Trump Organization, a global real estate developer.\n",
      "Trump was born on June 14, 1946\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"/pasteur/u/yuhuiz/data/LLAMA2/llama\")\n",
    "from llama.tokenizer import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(\"/pasteur/u/yuhuiz/data/LLAMA2/tokenizer.model\")\n",
    "\n",
    "\n",
    "def sample_top_p(probs, p):\n",
    "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "    mask = probs_sum - probs_sort > p\n",
    "    probs_sort[mask] = 0.0\n",
    "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "    next_token = torch.gather(probs_idx, -1, next_token)\n",
    "    return next_token\n",
    "\n",
    "\n",
    "def generate_top_p_language(\n",
    "    prefix: str, temperature: float = 0.6, top_p: float = 0.9, max_len: int = 128\n",
    "):\n",
    "    input_tokens = tokenizer.encode(prefix, bos=True, eos=False)\n",
    "    tokens = torch.tensor(input_tokens).unsqueeze(0).to(device)\n",
    "\n",
    "    for i in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            logits, _ = model(tokens)\n",
    "        if temperature > 0:\n",
    "            probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n",
    "            next_token = sample_top_p(probs, top_p)\n",
    "        else:\n",
    "            next_token = torch.argmax(logits[:, -1], dim=-1, keepdim=True)\n",
    "        tokens = torch.cat([tokens, next_token], dim=-1)\n",
    "\n",
    "    generation = tokenizer.decode(tokens[0].cpu().numpy().tolist())\n",
    "    return generation\n",
    "\n",
    "\n",
    "prompts = [\n",
    "    # For these prompts, the expected answer is the natural continuation of the prompt\n",
    "    \"I believe the meaning of life is\",\n",
    "    \"Simply put, the theory of relativity states that \",\n",
    "    \"\"\"A brief message congratulating the team on the launch:\n",
    "\n",
    "    Hi everyone,\n",
    "    \n",
    "    I just \"\"\",\n",
    "    # Few shot prompt (providing a few examples before asking model to complete more);\n",
    "    \"\"\"Translate English to French:\n",
    "    \n",
    "    sea otter => loutre de mer\n",
    "    peppermint => menthe poivrée\n",
    "    plush girafe => girafe peluche\n",
    "    cheese =>\"\"\",\n",
    "    \"\"\"He -> Him, She -> Her, They ->\"\"\",\n",
    "    \"\"\"Who is Donald Trump?\"\"\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(\"====================================\")\n",
    "    generated_text = generate_top_p_language(prompt)\n",
    "    print(prompt)\n",
    "    print(generated_text)\n",
    "    print(\"====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modeldiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
