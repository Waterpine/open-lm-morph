{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "\n",
    "sys.path.append(\"../../../open_lm\")\n",
    "from open_lm.model import Transformer\n",
    "from open_lm.norms import RmsNorm\n",
    "\n",
    "device = \"cuda:0\"\n",
    "cfg = json.load(open(\"../model_configs/l7b_llama.json\"))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Params:\n",
    "    dim: int\n",
    "    n_layers: int\n",
    "    n_heads: int\n",
    "    vocab_size: int\n",
    "    norm_eps: float\n",
    "    seq_len: int\n",
    "    post_embed_norm: bool\n",
    "    weight_tying: bool\n",
    "    norm_type: nn.Module = RmsNorm  # Make sure to use RmsNorm for LLaMA\n",
    "    apply_qk_norm: bool = False\n",
    "    rotary_old: bool = False\n",
    "    ffn_type: str = \"swiglu\"\n",
    "    llama: bool = True  # Make sure to set this to True for LLaMA\n",
    "\n",
    "\n",
    "args = Params(\n",
    "    dim=cfg[\"hidden_dim\"],\n",
    "    n_layers=cfg[\"n_layers\"],\n",
    "    n_heads=cfg[\"n_heads\"],\n",
    "    seq_len=cfg[\"seq_len\"],\n",
    "    vocab_size=cfg[\"vocab_size\"],\n",
    "    post_embed_norm=cfg[\"post_embed_norm\"],\n",
    "    weight_tying=cfg[\"weight_tying\"],\n",
    "    norm_eps=1e-5,\n",
    ")\n",
    "\n",
    "model = Transformer(args)\n",
    "state_dict = torch.load(\n",
    "    \"/pasteur/u/yuhuiz/data/LLAMA2/llama-2-7b/consolidated.00.converted.pth\"\n",
    ")\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "model = model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================\n",
      "I believe the meaning of life is\n",
      "I believe the meaning of life is to live a life of meaning.\n",
      "I believe the meaning of life is to be happy.\n",
      "I believe the meaning of life is to be a good person.\n",
      "I believe the meaning of life is to love.\n",
      "I believe the meaning of life is to be loved.\n",
      "I believe the meaning of life is to be loved by God.\n",
      "I believe the meaning of life is to live a life of meaning.\n",
      "I believe the meaning of life is to be happy.\n",
      "I believe the meaning of life is to be a good person.\n",
      "I believe the meaning of life is to love.\n",
      "I believe the meaning of\n",
      "====================================\n",
      "====================================\n",
      "Simply put, the theory of relativity states that \n",
      "Simply put, the theory of relativity states that 1) the speed of light is constant and 2) the laws of physics are the same for all observers. In a nutshell, the theory states that the laws of physics are the same for all observers.\n",
      "This is a pretty interesting theory that has been around for a long time. It’s not a new idea, but it’s one that has been around for a long time.\n",
      "I’m not sure if I’m the only one who thinks that the theory of relativity is a bit of a stretch. It’s not a new idea, but it’s one that has been around\n",
      "====================================\n",
      "====================================\n",
      "A brief message congratulating the team on the launch:\n",
      "\n",
      "    Hi everyone,\n",
      "    \n",
      "    I just \n",
      "A brief message congratulating the team on the launch:\n",
      "\n",
      "    Hi everyone,\n",
      "    \n",
      "    I just  wanted to take a minute to congratulate you on the launch of your new website!  I think it looks great and I can't wait to share it with my friends.  I know you've been working hard on it for a long time, so I hope you're proud of what you've accomplished.\n",
      "    \n",
      "    I'm looking forward to the next phase of the project, and I hope you are too!  Keep up the great work!\n",
      "    \n",
      "    Sincerely,\n",
      "    \n",
      "    John Doe\n",
      "\n",
      "### Welcome Email\n",
      "\n",
      "    Dear {first_\n",
      "====================================\n",
      "====================================\n",
      "Translate English to French:\n",
      "    \n",
      "    sea otter => loutre de mer\n",
      "    peppermint => menthe poivrée\n",
      "    plush girafe => girafe peluche\n",
      "    cheese =>\n",
      "Translate English to French:\n",
      "    \n",
      "    sea otter => loutre de mer\n",
      "    peppermint => menthe poivrée\n",
      "    plush girafe => girafe peluche\n",
      "    cheese => fromage\n",
      "    coral => corail\n",
      "    paper => papier\n",
      "    hare => lapin\n",
      "    marmalade => marmelade\n",
      "    carrot => carotte\n",
      "    ham => jambon\n",
      "    penguin => pinguin\n",
      "    cat => chat\n",
      "    sheep => mouton\n",
      "    lemon => citron\n",
      "    pear => poire\n",
      "    plum => prune\n",
      "    cucumber => concombre\n",
      "    chocolate => chocolat\n",
      "    ice cream => glace\n",
      "    cake => gâteau\n",
      "    almond => amande\n",
      "====================================\n",
      "====================================\n",
      "He -> Him, She -> Her, They ->\n",
      "He -> Him, She -> Her, They -> Them, It -> It\n",
      "I know the proper English grammar rules, but I'm not sure what's the proper way to use the pronouns.\n",
      "For example, I know that the pronouns \"He\" and \"Him\" are used for singular subjects, but what about \"They\" and \"Them\"?\n",
      "If I'm using \"They\" or \"Them\", is it referring to a singular subject or a plural subject?\n",
      "grammar pronouns\n",
      "You can use \"they\" and \"them\" for singular subjects. It's not that uncommon. –\n",
      "====================================\n",
      "====================================\n",
      "Who is Donald Trump?\n",
      "Who is Donald Trump? - by Bob Woodward (Paperback)\n",
      "The Washington Post’s Bob Woodward’s new book Fear: Trump in the White House is a deep dive into the inner workings of the Trump administration. The book is based on hundreds of hours of interviews with first-hand sources, meeting notes, personal diaries, files and documents. It is the most intimate exploration yet of the Trump presidency and the first inside look at Donald Trump’s White House.\n",
      "Fear is the most intimate portrait of a sitting president ever published during the president’s first years in office. Based on\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"/pasteur/u/yuhuiz/data/LLAMA2/llama\")\n",
    "from llama.tokenizer import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(\"/pasteur/u/yuhuiz/data/LLAMA2/tokenizer.model\")\n",
    "\n",
    "\n",
    "def sample_top_p(probs, p):\n",
    "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "    mask = probs_sum - probs_sort > p\n",
    "    probs_sort[mask] = 0.0\n",
    "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "    next_token = torch.gather(probs_idx, -1, next_token)\n",
    "    return next_token\n",
    "\n",
    "\n",
    "def generate_top_p_language(\n",
    "    prefix: str, temperature: float = 0.6, top_p: float = 0.9, max_len: int = 128\n",
    "):\n",
    "    input_tokens = tokenizer.encode(prefix, bos=True, eos=False)\n",
    "    tokens = torch.tensor(input_tokens).unsqueeze(0).to(device)\n",
    "\n",
    "    for i in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            logits, _ = model(tokens)\n",
    "        if temperature > 0:\n",
    "            probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n",
    "            next_token = sample_top_p(probs, top_p)\n",
    "        else:\n",
    "            next_token = torch.argmax(logits[:, -1], dim=-1, keepdim=True)\n",
    "        tokens = torch.cat([tokens, next_token], dim=-1)\n",
    "\n",
    "    generation = tokenizer.decode(tokens[0].cpu().numpy().tolist())\n",
    "    return generation\n",
    "\n",
    "\n",
    "prompts = [\n",
    "    # For these prompts, the expected answer is the natural continuation of the prompt\n",
    "    \"I believe the meaning of life is\",\n",
    "    \"Simply put, the theory of relativity states that \",\n",
    "    \"\"\"A brief message congratulating the team on the launch:\n",
    "\n",
    "    Hi everyone,\n",
    "    \n",
    "    I just \"\"\",\n",
    "    # Few shot prompt (providing a few examples before asking model to complete more);\n",
    "    \"\"\"Translate English to French:\n",
    "    \n",
    "    sea otter => loutre de mer\n",
    "    peppermint => menthe poivrée\n",
    "    plush girafe => girafe peluche\n",
    "    cheese =>\"\"\",\n",
    "    \"\"\"He -> Him, She -> Her, They ->\"\"\",\n",
    "    \"\"\"Who is Donald Trump?\"\"\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(\"====================================\")\n",
    "    generated_text = generate_top_p_language(prompt)\n",
    "    print(prompt)\n",
    "    print(generated_text)\n",
    "    print(\"====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modeldiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
